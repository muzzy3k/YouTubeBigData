{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7bf94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e57b551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/24 02:14:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/24 02:14:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"YouTube_Model_GB\")\n",
    "        # allocate 8 GB to the driver JVM:\n",
    "        .config(\"spark.driver.memory\", \"8g\")\n",
    "        # when running in local mode this also inflates the executor memory:\n",
    "        .config(\"spark.executor.memory\", \"8g\")\n",
    "        # limit the size of results Spark will try to pull back to Python\n",
    "        .config(\"spark.driver.maxResultSize\", \"2g\")\n",
    "        .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7754c8ca-cb84-49be-b8cd-e5fe6fce58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to hide long warning messages in console output\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2814e47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "import os\n",
    "current_path = os.getcwd()\n",
    "full_path = f'file://{current_path}/GB_youtube_trending_data_clean.csv'\n",
    "df = spark.read.csv(full_path, inferSchema = True, header = True)\n",
    "df = df.drop(\"_c15\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bff6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(F.col(\"view_count\") != 0)\n",
    "df = df.filter(F.col(\"comments_disabled\") != 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78721aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding title statistics and seeing statistics \n",
    "# Create a column for the length of each title (number of characters)\n",
    "df = df.withColumn(\"title_length\", F.length(\"title\"))\n",
    "\n",
    "# Create a column for the word count (splitting on whitespace)\n",
    "df = df.withColumn(\"word_count\", F.size(F.split(\"title\", \" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfce5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"publish_year\", F.year(\"publishedAt\")).withColumn(\"publish_month\", F.month(\"publishedAt\")).withColumn(\"publish_dayofweek\", F.dayofweek(\"publishedAt\")).withColumn(\"publish_hour\", F.hour(\"publishedAt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d43d9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Catagorical columns\n",
    "# mapping categoryid to its value through joining json file\n",
    "categories_df = spark.read.option(\"multiLine\", \"true\").json(\"GB_category_id.json\")\n",
    "categories_df = categories_df.select(F.explode(\"items\").alias(\"item\"))\n",
    "categories_df = categories_df.select(\n",
    "    F.col(\"item.id\").alias(\"categoryId\"),\n",
    "    F.col(\"item.snippet.title\").alias(\"categoryTitle\")\n",
    ")\n",
    "\n",
    "# Cast categoryId to integer so it matches your main DataFrame\n",
    "categories_df = categories_df.withColumn(\"categoryId\", F.col(\"categoryId\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43d7493e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_categories = df.join(categories_df, on=\"categoryId\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c70ec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"dislikes_ratio\", F.round(F.col(\"dislikes\") / F.col(\"view_count\"), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "598dce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"comments_ratio\", F.col(\"comment_count\")/F.col(\"view_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da95225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"likes_ratio\", F.round(F.col(\"likes\") / F.col(\"view_count\"), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b716a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags_split = df.withColumn(\"tag_array\", F.split(F.col(\"tags\"), r\"\\|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb2d0718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.ml.feature import HashingTF, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define UDF to lowercase each element in the tag array\n",
    "def lower_array(tags):\n",
    "    if tags is None:\n",
    "        return []\n",
    "    return [tag.lower() for tag in tags]\n",
    "\n",
    "lower_udf = F.udf(lower_array, ArrayType(StringType()))\n",
    "\n",
    "# Create a new DataFrame so that original data is not overwritten.\n",
    "# Lowercase the tag array column and store it in a new column 'tag_array_lower'\n",
    "df_ml = df_tags_split.withColumn(\"tag_array_lower\", lower_udf(F.col(\"tag_array\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32e15b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the selected numerical columns to string so we can treat them as categorical\n",
    "df_ml = df_ml.withColumn(\"categoryID_str\", F.col(\"categoryId\").cast(\"string\")) \\\n",
    "             .withColumn(\"title_length_str\", F.col(\"title_length\").cast(\"string\")) \\\n",
    "             .withColumn(\"word_count_str\", F.col(\"word_count\").cast(\"string\")) \\\n",
    "             .withColumn(\"publish_month_str\", F.col(\"publish_month\").cast(\"string\")) \\\n",
    "             .withColumn(\"publish_dayofweek_str\", F.col(\"publish_dayofweek\").cast(\"string\")) \\\n",
    "             .withColumn(\"publish_hour_str\", F.col(\"publish_hour\").cast(\"string\"))\n",
    "\n",
    "# Define the list of our categorical columns (as strings)\n",
    "cat_cols = [\"categoryID_str\", \"title_length_str\", \"word_count_str\", \n",
    "            \"publish_month_str\", \"publish_dayofweek_str\", \"publish_hour_str\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1802986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 4) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tags: 215734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# getting unique tags to use as buckets in hashingtf\n",
    "unique_tag_count = df_ml.select(F.explode(F.col(\"tag_array_lower\"))).distinct().count()\n",
    "print(\"Unique tags:\", unique_tag_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb026fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each categorical column, create a StringIndexer (with handleInvalid=\"keep\" to avoid errors)\n",
    "indexers = [StringIndexer(inputCol=col_name, outputCol=col_name + \"_idx\", handleInvalid=\"keep\")\n",
    "            for col_name in cat_cols]\n",
    "\n",
    "# Next, use a OneHotEncoder to convert the indexed categories to one-hot encoded vectors\n",
    "encoders = [OneHotEncoder(inputCol=col_name + \"_idx\", outputCol=col_name + \"_ohe\")\n",
    "            for col_name in cat_cols]\n",
    "\n",
    "# Create the HashingTF stage to convert the preprocessed tag array into a fixed-length feature vector.\n",
    "hashing_tf = HashingTF(inputCol=\"tag_array_lower\", outputCol=\"hashing_tf_features\", numFeatures=262144)\n",
    "\n",
    "# Assemble all features into a single vector. We use the HashingTF features\n",
    "# as well as all the one-hot encoded categorical columns.\n",
    "assembler_inputs = [\"hashing_tf_features\"] + [col_name + \"_ohe\" for col_name in cat_cols]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "# Create the pipeline by combining all stages\n",
    "pipeline_stages = []\n",
    "pipeline_stages += indexers\n",
    "pipeline_stages += encoders\n",
    "pipeline_stages.append(hashing_tf)\n",
    "pipeline_stages.append(assembler)\n",
    "\n",
    "pipeline = Pipeline(stages=pipeline_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ef144c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the target variable 'comments_ratio' to 'label' for cr modleing\n",
    "df_ml_cr = df_ml.withColumnRenamed(\"comments_ratio\", \"label\")\n",
    "\n",
    "# Fit the pipeline model on the data and transform the DataFrame.\n",
    "pipeline_model = pipeline.fit(df_ml_cr)\n",
    "df_ml_cr_final = pipeline_model.transform(df_ml_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4a420a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for target: comments_ratio\n",
      "Test RMSE: 0.002735034034659714\n",
      "Test R2: 0.6651347058170651\n"
     ]
    }
   ],
   "source": [
    "# linear rgreression predict comments ratio\n",
    "\n",
    "train, test = df_ml_cr_final.randomSplit([0.7, 0.3], seed=101)\n",
    "\n",
    "# Build a Linear Regression model to predict the label (comments_ratio)\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model_cr = lr.fit(train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = lr_model_cr.evaluate(test)\n",
    "print(\"Building model for target: comments_ratio\")\n",
    "print(\"Test RMSE:\", test_results.rootMeanSquaredError)\n",
    "print(\"Test R2:\", test_results.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be2a4174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model for target: likes_ratio\n",
      "Test RMSE: 0.018117121162638337\n",
      "Test R2: 0.732989953602224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 44276)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/james/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/james/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/home/james/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/james/.local/lib/python3.8/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# likes ratio for modeling\n",
    "df_ml_lr = df_ml.withColumnRenamed(\"likes_ratio\", \"label\")\n",
    "\n",
    "# Fit the pipeline model on the data and transform the DataFrame.\n",
    "pipeline_model = pipeline.fit(df_ml_lr)\n",
    "df_ml_lr_final = pipeline_model.transform(df_ml_lr)\n",
    "\n",
    "# linear rgreression predict likes ratio\n",
    "\n",
    "train, test = df_ml_lr_final.randomSplit([0.7, 0.3], seed=101)\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model_lr = lr.fit(train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = lr_model_lr.evaluate(test)\n",
    "print(\"Building model for target: likes_ratio\")\n",
    "print(\"Test RMSE:\", test_results.rootMeanSquaredError)\n",
    "print(\"Test R2:\", test_results.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dceb3f-036a-4603-b24e-3fcc66b3e8dd",
   "metadata": {},
   "source": [
    "### COunt Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "589fc82d-2113-4329-9a9a-eab8a7d14ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments_ratio → CountVec:  RMSE = 0.002634519198063319 , R2 = 0.6785895938647523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 240:==============>                                          (1 + 3) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes_ratio    → CountVec:  RMSE = 0.018071296008784467 , R2 = 0.735822728077397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Build the new pipeline: CountVectorizer + OneHot + Assembler\n",
    "from pyspark.ml.feature import CountVectorizer, StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# CountVectorizer on tags  minDF=5\n",
    "cv = CountVectorizer(inputCol=\"tag_array_lower\", outputCol=\"tagVec\",\n",
    "                     vocabSize=215734, minDF=0) # keep every tag\n",
    "\n",
    "# index & OHE for our categorical strs\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "encoders  = [OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_ohe\")          for c in cat_cols]\n",
    "\n",
    "# assemble\n",
    "assembler_inputs = [\"tagVec\"] + [c+\"_ohe\" for c in cat_cols]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[cv] + indexers + encoders + [assembler])\n",
    "\n",
    "\n",
    "# Model for comments_ratio\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "df_ml_cr    = df_ml.withColumnRenamed(\"comments_ratio\", \"label\")\n",
    "pipe_model  = pipeline.fit(df_ml_cr)\n",
    "df_cr_final = pipe_model.transform(df_ml_cr)\n",
    "\n",
    "train_cr, test_cr = df_cr_final.randomSplit([0.7,0.3], seed=101)\n",
    "lr_cr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model_cr = lr_cr.fit(train_cr)\n",
    "res_cr      = lr_model_cr.evaluate(test_cr)\n",
    "\n",
    "print(\"Comments_ratio → CountVec:  RMSE =\", res_cr.rootMeanSquaredError,\n",
    "      \", R2 =\", res_cr.r2)\n",
    "\n",
    "# Model for likes_ratio\n",
    "df_ml_lr    = df_ml.withColumnRenamed(\"likes_ratio\", \"label\")\n",
    "df_lr_final = pipe_model.transform(df_ml_lr)\n",
    "\n",
    "train_lr, test_lr = df_lr_final.randomSplit([0.7,0.3], seed=101)\n",
    "lr_lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model_lr = lr_lr.fit(train_lr)\n",
    "res_lr      = lr_model_lr.evaluate(test_lr)\n",
    "\n",
    "print(\"Likes_ratio    → CountVec:  RMSE =\", res_lr.rootMeanSquaredError,\n",
    "      \", R2 =\", res_lr.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1edc7f-10af-4151-92eb-f7cb8a55d8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
